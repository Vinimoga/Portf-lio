{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a neural network theory review, so this way, I can expand my memory and reasoning, and at the same time be with a memory book on my back, so lets start this.\n",
    "\n",
    "The Series of video I've searched o learn about this is this one: [Neural Networks from Scratch](https://www.youtube.com/watch?v=Wo5dMEP_BbI), where he tries to teach in full basic python and then uses only numpy (witch is self explicatory).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cap. 1 - Introduction to Neural Networks\n",
    "\n",
    "Let's start with what is neural networks, \n",
    "\n",
    "Neural networks are a class of machine learning models inspired by the structure and functioning of the human brain. They are composed of interconnected nodes, or artificial neurons, organized in layers.\n",
    "\n",
    "Each neuron takes multiple input values, performs a weighted sum of these inputs, applies an activation function to the result, and produces an output. The weights associated with each input are adjusted during training to enable the network to learn from data.\n",
    "\n",
    "[Example of a neural Network](https://www.youtube.com/watch?v=-6mZWjIEkDc&t=1s)\n",
    "\n",
    "Data is going to get passed forward until we have an oputput layer with will output a probbility of with one would be. On the example, it is the probability of being a dog or a cat.\n",
    "\n",
    "So let's do an example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.08\n"
     ]
    }
   ],
   "source": [
    "#Here we are representing one neuron. It is receiving imputs from the last layer and have weight to each one of them\n",
    "#after we are going to sum a bias (explained after)\n",
    "inputs = [1.2, 5.3, 4.1]\n",
    "weights = [3.1, 2.2, 9.0]\n",
    "bias = [0.8]\n",
    "\n",
    "output = inputs[0] * weights[0] + inputs[1] * weights[1] + inputs[2] * weights[2] + bias[0]\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This](https://www.youtube.com/watch?v=vbeanwfm0Q4) is visualy what the neuron is actually doing.\n",
    "\n",
    "This is calculation of what a neuron should output, next we are going to se how to see it in linear algebra..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cap. 2 - Coding a layer\n",
    "\n",
    "for us to see the layer, let's think of it being a conjunction of neurons. Each neuron have a series of weights that will multiply the input, for this what we can do is to represent the layer with a list of lists, each one of them being the set o weights of one neuron, and doing the same thing with the bias.\n",
    "\n",
    "[Example of a layer](https://www.youtube.com/watch?v=Uvngs6sWyBg&t=2s)\n",
    "\n",
    "We can see now that the output will be a series of answers of each neuron, so we will have a list of answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[57.715, 41.27, 98.5]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Here we are representing one layer with multiple neurons.\n",
    "inputs = [1.2, 5.3, 4.5]\n",
    "\n",
    "weights1 = [3.1, 2.2, 9.23]\n",
    "weights2 = [2.5, 5.4, 1.7]\n",
    "weights3 = [4.2, 10.2, 8.4]\n",
    "\n",
    "layerweight = [weights1, weights2, weights3]\n",
    "\n",
    "bias = [0.8, 2, 1.6]\n",
    "\n",
    "output = [inputs[0] * layerweight[0][0] + inputs[1] * layerweight[0][1] + inputs[2] * layerweight[0][2] + bias[0],\n",
    "          inputs[0] * layerweight[1][0] + inputs[1] * layerweight[1][1] + inputs[2] * layerweight[1][2] + bias[1],\n",
    "          inputs[0] * layerweight[2][0] + inputs[1] * layerweight[2][1] + inputs[2] * layerweight[2][2] + bias[2]]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simplify things doing linear algebra, as we can see, que can do the dot product into the vectors inputs and weights to het the same exact answer.\n",
    "\n",
    "For this, we are going to use numpy and transform all the vector into numpy arrays and do the dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57.715 41.27  98.5  ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "inputs = np.array([1.2, 5.3, 4.5])\n",
    "\n",
    "weights1 = [3.1, 2.2, 9.23]\n",
    "weights2 = [2.5, 5.4, 1.7]\n",
    "weights3 = [4.2, 10.2, 8.4]\n",
    "\n",
    "layerweight = np.array([weights1, weights2, weights3])\n",
    "\n",
    "biases = np.array([0.8, 2, 1.6])\n",
    "\n",
    "layer_output = np.zeros((len(layerweight)))\n",
    "\n",
    "for i in range(len(inputs)):\n",
    "    output = 0\n",
    "    for j in range(len(layerweight)):\n",
    "        output += inputs[j] * layerweight[i][j]\n",
    "        #print(i,j,output)\n",
    "    output += biases[i]\n",
    "    #print(output)\n",
    "    layer_output[i] = output\n",
    "\n",
    "print(layer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we iterate through each one of the elements on the numpy array and did the operations and then appended the result to the layer output.\n",
    "\n",
    "We can have in another way, did in the series of videos in witch I'll write down for you to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57.715, 41.27, 98.5]\n"
     ]
    }
   ],
   "source": [
    "#The same thing but doing a diferent way of solving it (or explaning)\n",
    "import numpy as np \n",
    "\n",
    "inputs = np.array([1.2, 5.3, 4.5])\n",
    "\n",
    "weights1 = [3.1, 2.2, 9.23]\n",
    "weights2 = [2.5, 5.4, 1.7]\n",
    "weights3 = [4.2, 10.2, 8.4]\n",
    "\n",
    "layerweight = np.array([weights1, weights2, weights3])\n",
    "\n",
    "biases = np.array([0.8, 2, 1.6])\n",
    "\n",
    "layer_output = []\n",
    "for neuron_weights, neuron_bias in zip(layerweight, biases):\n",
    "    neuron_output = 0\n",
    "    for n_input, weight in zip(inputs, neuron_weights):\n",
    "        neuron_output += n_input*weight\n",
    "    neuron_output += neuron_bias\n",
    "    layer_output.append(neuron_output)\n",
    "\n",
    "print(layer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can already verify or understand because of linear algebra, what we did is the dot product, wicth we can put it on the code to really simplify our understanding:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57.715 41.27  98.5  ]\n"
     ]
    }
   ],
   "source": [
    "#The same thing but doing a diferent way of solving it (or explaning)\n",
    "import numpy as np \n",
    "\n",
    "inputs = np.array([1.2, 5.3, 4.5])\n",
    "\n",
    "weights1 = [3.1, 2.2, 9.23]\n",
    "weights2 = [2.5, 5.4, 1.7]\n",
    "weights3 = [4.2, 10.2, 8.4]\n",
    "\n",
    "layerweight = np.array([weights1, weights2, weights3])\n",
    "\n",
    "biases = np.array([0.8, 2, 1.6])\n",
    "\n",
    "layer_output = np.dot(layerweight, inputs) + biases\n",
    "\n",
    "print(layer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cap.3 - batches, layers and objects\n",
    "\n",
    "we already dealt with one neuron and one input and one layer and one input, but now we want to extrapolate, image two layers, five, ten, imagine each layer with 50 neuron, 100, 1000.\n",
    "\n",
    "for this we need to imaginate all the things into objects, for simplification.\n",
    "\n",
    "But why we should need to send more than one input at a time, and this is simple, we can do calculations in paralel, witch can have a huge effect when we think about time spent training models. The second reason is what you can see in this [video](https://nnfs.io/vyu/), as we give more batch sizes, the less wooble the curve has and the better it can recognize the pattern.\n",
    "\n",
    "Being carefull to not do overfitting, that is, where a model learns the training data too well, including its noise and specific patterns that may not generalize well to new, unseen data. In other words, an overfit model fits the training data too closely, capturing the noise or random fluctuations in the data rather than the underlying patterns.\n",
    "\n",
    "For this, let's upgrade the last code giving more input data, for this, we need to change only one thing, being the dot product. As we saw on the last class the dot product helps immensily on seing what is happening behind the curtains, but now with an input array with more than 1 dimentions we have a problem, witch is simply solved by transposing the second array, being the weights. this because [how the matrix multiplication works](https://www.youtube.com/watch?v=KBPvlUp-m5Y). \n",
    "\n",
    "Let's see the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "layerweight = [[0.2, 0.8, -0.5, 1.0],\n",
    "               [0.5, -0.91, 0.26, -0.5],\n",
    "               [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "layer_output = np.dot(inputs, np.array(layerweight).T) + biases\n",
    "\n",
    "print(layer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This](https://www.youtube.com/watch?v=ocrXqFCW3WE&t=31s) is excatly what we did\n",
    "\n",
    "Now for us to have another layer, we need to have another set of neuron, with another set of weight and biases sinc with it, for this, see the code below\n",
    "\n",
    "Note that the input of the layer2 is the layer_output of the layer1 giving the network it's connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "#The same thing but doing a diferent way of solving it (or explaning)\n",
    "import numpy as np \n",
    "\n",
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "layerweight1 = [[0.2, 0.8, -0.5, 1.0],\n",
    "               [0.5, -0.91, 0.26, -0.5],\n",
    "               [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "layerweight2 = [[0.1, -0.14, 0.5],\n",
    "                [-0.5, 0.12, -0.33],\n",
    "                [-0.44, 0.73, -0.13]]\n",
    "\n",
    "biases1 = [2, 3, 0.5]\n",
    "\n",
    "biases2 = [-1, 2, -0.5]\n",
    "\n",
    "layer_output1 = np.dot(inputs, np.array(layerweight1).T) + biases1\n",
    "\n",
    "layer_output2 = np.dot(layer_output1, np.array(layerweight2).T) + biases2\n",
    "\n",
    "print(layer_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can think of the layer as a object itself, because, we saw that we need to have 2 things to create a layer: it's shape and it's number of inputs, because with this, we can create a new array with biases and weights and we can create function tha gives the output given their inputs, so we can create a class like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NetworkLayer():\n",
    "    '''\n",
    "    In the init function we need to create a set o weights and a set of biases, for the weights we know \n",
    "    (because of the dot product) that it need to be number imput (or else it doesn't match) by the number \n",
    "    of neurons in the layer, and for the biases it need to be 1 by the number of imputs (each imput with\n",
    "    one bias).\n",
    "    '''\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    '''\n",
    "    for the forward method we need simply to do the calculations and give the result in a way that can be \n",
    "    used somewhere else in the code for new layers to use, so we simple give a variable name or we return\n",
    "    it.\n",
    "    '''\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can identify and create layers of neurons in our network and cascate it for more layers in our deep lerning network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09641592 -0.45234704  0.56572563 -0.11232398 -0.1293504 ]\n",
      " [-0.43472621 -0.9388246   0.36680802 -0.30675596 -0.8060944 ]\n",
      " [ 0.63989101 -0.07786409  0.55203872 -0.39003505 -0.22158323]]\n"
     ]
    }
   ],
   "source": [
    "#example with one layer\n",
    "import numpy as np \n",
    "\n",
    "X = [[1, 2, 3, 2.5],\n",
    "     [2.0, 5.0, -1.0, 2.0],\n",
    "     [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "layer = NetworkLayer(4,5)\n",
    "\n",
    "layer.forward(X)\n",
    "print(layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights [[-0.12928569  0.02670509 -0.00392828 -0.11680935  0.05232767]\n",
      " [-0.01715463  0.07717906  0.08235042  0.21632359  0.13365279]]\n",
      "\n",
      "biases [[0. 0. 0. 0. 0.]]\n",
      "\n",
      "output [[-0.16359496  0.1810632   0.16077255  0.31583784  0.31963326]\n",
      " [-0.34434455  0.43930545  0.40389551  0.84799928  0.77291931]\n",
      " [ 0.14761103  0.16832582  0.22823854  0.75928773  0.28237105]]\n",
      "\n",
      "weights [[-0.03691818]\n",
      " [-0.02393792]\n",
      " [ 0.10996596]\n",
      " [ 0.06552637]\n",
      " [ 0.06401315]]\n",
      "\n",
      "biases [[0.]]\n",
      "\n",
      "output [[0.0605413 ]\n",
      " [0.15165459]\n",
      " [0.0834484 ]]\n"
     ]
    }
   ],
   "source": [
    "#example with two layers\n",
    "import numpy as np \n",
    "\n",
    "X = [[1, 2],\n",
    "     [2.0, 5.0],\n",
    "     [-1.5, 2.7]]\n",
    "\n",
    "first_layer = NetworkLayer(2,5)\n",
    "second_layer = NetworkLayer(5,1)\n",
    "\n",
    "first_layer.forward(X)\n",
    "print('weights',first_layer.weights)\n",
    "print()\n",
    "print('biases', first_layer.biases)\n",
    "print()\n",
    "print('output', first_layer.output)\n",
    "\n",
    "print()\n",
    "\n",
    "second_layer.forward(first_layer.output)\n",
    "print('weights',second_layer.weights)\n",
    "print()\n",
    "print('biases', second_layer.biases)\n",
    "print()\n",
    "print('output', second_layer.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the neural network we are representing right now:\n",
    "\n",
    "![Example of the Neural Network](Neural_Network_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cap.4 -  Hidden Layer Activation Functions\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
